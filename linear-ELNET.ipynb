{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "413r0PzfKZGM"
   },
   "source": [
    "**Linear regression with elastic net penalty**\n",
    "\n",
    "\n",
    "The goal of the following code is to compute the bias, variance and MSE of using LO to estimate the out-out-of-error (OO) as a function of $p$ (and $n = \\delta p$). The theory claims that\n",
    "$$\n",
    "MSE \\sim var \\sim O(1/n)=O(1/p).\n",
    "$$\n",
    "\n",
    "To do so we sample synthetic data and numerically regress $\\log(MSE)$ (and $\\log$(var)  against $\\log p$ and report the $R^2$ and the scaling exponent, eg. $ MSE \\sim C/n^a$ or $\\log MSE = \\log C + a \\log n$.\n",
    "\n",
    "The $\\lambda$ used in the code scales like $1/n$ as $n,p$ changes because the equation optimized in the `ElasticNetCV` function when fitting the model is:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^n (y_i - x_i^\\top \\beta)^2 + \\lambda \\left( \\alpha \\|\\beta\\|_1 + \\frac{1}{2} (1 - \\alpha) \\|\\beta\\|^2_2 \\right) \\right\\}\n",
    "$$\n",
    "\n",
    "where $ \\lambda $ is the regularization parameter, $ \\alpha $ is the `l1_ratio` mixing parameter between L1 and L2 penalties, $ n $ is the number of samples, $ y_i $ are the observed values, $ x_i $ are the predictors, and $ \\beta$ are the coefficients to be estimated.\n",
    "\n",
    "\n",
    "\n",
    "The reported $df$ is to make sure that the selected $\\lambda$ is reasonable, in the mid-range giving us a number of estimated coefficients that we expect.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vUvr6yUIeBQ",
    "outputId": "ac49d1c8-6298-4330-83b9-cc2a4a3de910"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnab\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\arnab\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\arnab\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p=  120| s=100| df=0.80, lmb=1.0| lo=0.59| outErr=0.59| MSE =0.006 | scaling = 0.00 | R2_MSE = 0.00\n",
      "p=  120| s=200| df=1.02, lmb=1.0| lo=0.73| outErr=0.59| MSE =0.006 | scaling = 0.00 | R2_MSE = 0.00\n",
      "p=  190| s=100| df=1.17, lmb=1.0| lo=0.68| outErr=0.58| MSE =0.003 | scaling = -1.55 | R2_MSE = 1.00\n",
      "p=  190| s=200| df=0.84, lmb=1.0| lo=0.52| outErr=0.58| MSE =0.003 | scaling = -1.29 | R2_MSE = 1.00\n",
      "p=  301| s=100| df=1.03, lmb=1.0| lo=0.62| outErr=0.59| MSE =0.003 | scaling = -0.93 | R2_MSE = 0.95\n",
      "p=  301| s=200| df=1.16, lmb=1.0| lo=0.69| outErr=0.60| MSE =0.002 | scaling = -1.15 | R2_MSE = 0.99\n",
      "p=  477| s=100| df=0.87, lmb=1.0| lo=0.63| outErr=0.58| MSE =0.001 | scaling = -1.20 | R2_MSE = 1.00\n",
      "p=  477| s=200| df=0.81, lmb=1.0| lo=0.55| outErr=0.59| MSE =0.001 | scaling = -1.11 | R2_MSE = 1.00\n",
      "p=  757| s=100| df=0.80, lmb=1.0| lo=0.56| outErr=0.58| MSE =0.001 | scaling = -1.08 | R2_MSE = 1.00\n",
      "p=  757| s=200| df=0.79, lmb=1.0| lo=0.59| outErr=0.58| MSE =0.001 | scaling = -1.04 | R2_MSE = 0.99\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from scipy.stats import laplace, norm\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Parameters\n",
    "#Haolin: I changed the array to equal space in log scale\n",
    "logp_       = np.arange(0,6)\n",
    "plogstep    = 10**(0.2)\n",
    "p_          = 120* plogstep**(logp_)\n",
    "p_          = p_.astype(int)\n",
    "delta       = 1.5 # n/p\n",
    "rho         = 0.2 # k/p\n",
    "alpha_elnet = 1 # l1_ratio, alpha_elnet=1 is L1,  alpha_elnet=0 is L2\n",
    "MCMCsamples = 200\n",
    "lo          = np.zeros((MCMCsamples, len(p_)))\n",
    "outErr      = np.zeros((MCMCsamples, len(p_)))\n",
    "MSE         = np.zeros(len(p_))\n",
    "bias2       = np.zeros(len(p_))\n",
    "var         = np.zeros(len(p_))\n",
    "o           = 1\n",
    "a           = 0\n",
    "R2          = 0\n",
    "pos         = False\n",
    "\n",
    "# Generate data and fit model\n",
    "for i, p in enumerate(p_):\n",
    "    #if(i < 5):\n",
    "    #    continue\n",
    "    n = int(p * delta)\n",
    "    k = int(p * rho)\n",
    "    for s in range(MCMCsamples):\n",
    "        #if(s < 21):\n",
    "        #    continue\n",
    "        if(i == 5 and s%10 == 0):\n",
    "            print(s)\n",
    "        beta_star = np.zeros(p)\n",
    "        beta_star[0:k] = 1 #laplace.rvs(scale=1/np.sqrt(2), size=k) * np.sqrt(n/k)\n",
    "        X = np.random.normal(0, 1/np.sqrt(n), (n, p))\n",
    "        y = X @ beta_star + norm.rvs(scale=o, size=n)\n",
    "        if alpha_elnet > 0:\n",
    "            lambdaS = np.array([1 / n])\n",
    "            model = ElasticNetCV(l1_ratio=alpha_elnet, tol=0.001, max_iter=10000, positive = pos, alphas=lambdaS, cv=n, fit_intercept=False).fit(X, y)\n",
    "            lo[s, i] = np.mean(model.mse_path_) / 2  # Adjust as needed;\n",
    "        else:\n",
    "            lambdaS = np.array([100 / n])\n",
    "            model = RidgeCV(alphas=lambdaS, store_cv_values=True, tol=0.001, max_iter=10000,  cv=None, fit_intercept=False).fit(X, y)\n",
    "            lo[s, i] = np.mean(model.cv_values_) / 2  # Adjust as needed;\n",
    "\n",
    "        beta_hat = model.coef_\n",
    "        outErr[s, i] = (o**2 + np.sum((beta_star - beta_hat) ** 2) / n) / 2\n",
    "        #np.count_nonzero(beta_hat)\n",
    "        df = np.mean(np.abs(beta_hat))/np.mean(np.abs(beta_star))\n",
    "        # Calculate bias and MSE\n",
    "        bias2[i] = (np.mean(outErr[:s+1, i] -lo[:s+1, i])) ** 2\n",
    "        MSE[i] = np.mean((lo[:s+1, i] - outErr[:s+1, i]) ** 2)\n",
    "        var[i] = MSE[i] - bias2[i]\n",
    "        if i > 0:\n",
    "            ls_fit = sm.OLS(np.log(MSE[:i+1]), sm.add_constant(np.log(p_[:i+1]))).fit()\n",
    "            a = ls_fit.params[1]\n",
    "            R2 = ls_fit.rsquared\n",
    "        if s % 100 == 99:  # Print every 100 iterations of s\n",
    "            print(f\"p= {p:4d}| s={s+1:3d}| df={df:.2f}, lmb={lambdaS[0]*n:.1f}| lo={lo[s, i]:.2f}| outErr={outErr[s, i]:.2f}| MSE ={MSE[i]:.3f} | scaling = {a:.2f} | R2_MSE = {R2:.2f}\")\n",
    "\n",
    "\n",
    "ls_mse_fit = sm.OLS(np.log(MSE), sm.add_constant(np.log(p_))).fit()\n",
    "a_mse = ls_mse_fit.params[1]\n",
    "R2_mse = ls_mse_fit.rsquared\n",
    "\n",
    "ls_var_fit = sm.OLS(np.log(var), sm.add_constant(np.log(p_))).fit()\n",
    "a_var = ls_var_fit.params[1]\n",
    "R2_var = ls_var_fit.rsquared\n",
    "\n",
    "ls_bias2_fit = sm.OLS(np.log(bias2), sm.add_constant(np.log(p_))).fit()\n",
    "a_bias2 = ls_bias2_fit.params[1]\n",
    "R2_bias2 = ls_bias2_fit.rsquared\n",
    "\n",
    "print(f\" MSE: scaling = {a_mse:.2f} | R2 = {R2_mse:.2f}\")\n",
    "print(f\" var: scaling = {a_var:.2f} | R2 = {R2_var:.2f}\")\n",
    "print(f\" bias2: scaling = {a_bias2:.2f} | R2 = {R2_bias2:.2f}\")\n",
    "\n",
    "\n",
    "# Calculate MSE standard error\n",
    "MSE_SE = np.std((lo - outErr) ** 2, axis=0) / np.sqrt(MCMCsamples)\n",
    "print(\"Mean squared errors:\", np.mean((lo - outErr) ** 2, axis=0))\n",
    "print(\"MSE standard errors:\", MSE_SE)\n",
    "\n",
    "results = np.column_stack((lo, outErr))\n",
    "\n",
    "np.savetxt(\"linear-noncons/lasso/delta1_5p120.csv\", results, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"linear-noncons/elnet/lo_delta0_5p200.csv\", lo, delimiter=\",\")\n",
    "#np.savetxt(\"linear-noncons/elnet/outErr_delta0_5p200.csv\", outErr, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_mse_fit = sm.OLS(np.log(MSE), sm.add_constant(np.log(p_))).fit()\n",
    "a_mse = ls_mse_fit.params[1]\n",
    "R2_mse = ls_mse_fit.rsquared\n",
    "\n",
    "ls_var_fit = sm.OLS(np.log(var), sm.add_constant(np.log(p_))).fit()\n",
    "a_var = ls_var_fit.params[1]\n",
    "R2_var = ls_var_fit.rsquared\n",
    "\n",
    "ls_bias2_fit = sm.OLS(np.log(bias2), sm.add_constant(np.log(p_))).fit()\n",
    "a_bias2 = ls_bias2_fit.params[1]\n",
    "R2_bias2 = ls_bias2_fit.rsquared\n",
    "\n",
    "print(f\" MSE: scaling = {a_mse:.2f} | R2 = {R2_mse:.2f}\")\n",
    "print(f\" var: scaling = {a_var:.2f} | R2 = {R2_var:.2f}\")\n",
    "print(f\" bias2: scaling = {a_bias2:.2f} | R2 = {R2_bias2:.2f}\")\n",
    "\n",
    "\n",
    "# Calculate MSE standard error\n",
    "MSE_SE = np.std((lo - outErr) ** 2, axis=0) / np.sqrt(MCMCsamples)\n",
    "print(\"Mean squared errors:\", np.mean((lo - outErr) ** 2, axis=0))\n",
    "print(\"MSE standard errors:\", MSE_SE)\n",
    "\n",
    "results = np.column_stack((lo, outErr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pbdwF-Ss6Fv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "ls_mse_fit = sm.OLS(np.log(MSE), sm.add_constant(np.log(p_))).fit()\n",
    "a_mse = ls_mse_fit.params[1]\n",
    "R2_mse = ls_mse_fit.rsquared\n",
    "\n",
    "ls_var_fit = sm.OLS(np.log(var), sm.add_constant(np.log(p_))).fit()\n",
    "a_var = ls_var_fit.params[1]\n",
    "R2_var = ls_var_fit.rsquared\n",
    "\n",
    "ls_bias2_fit = sm.OLS(np.log(bias2), sm.add_constant(np.log(p_))).fit()\n",
    "a_bias2 = ls_bias2_fit.params[1]\n",
    "R2_bias2 = ls_bias2_fit.rsquared\n",
    "\n",
    "print(f\" MSE: scaling = {a_mse:.2f} | R2 = {R2_mse:.2f}\")\n",
    "print(f\" var: scaling = {a_var:.2f} | R2 = {R2_var:.2f}\")\n",
    "print(f\" bias2: scaling = {a_bias2:.2f} | R2 = {R2_bias2:.2f}\")\n",
    "\n",
    "\n",
    "# Calculate MSE standard error\n",
    "MSE_SE = np.std((lo - outErr) ** 2, axis=0) / np.sqrt(MCMCsamples)\n",
    "print(\"Mean squared errors:\", np.mean((lo - outErr) ** 2, axis=0))\n",
    "print(\"MSE standard errors:\", MSE_SE)\n",
    "\n",
    "# Calculate logarithms\n",
    "log_bias2 = np.log(bias2)\n",
    "log_p_ = np.log(p_)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(p_, bias2, color='blue')\n",
    "plt.xlabel('p_')\n",
    "plt.ylabel('bias^2')\n",
    "plt.title('bias^2 vs p_')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BDaHH_lxW-2l"
   },
   "outputs": [],
   "source": [
    "#Added by Haolin: side by side boxplot for two groups of results, e.g. with and without constraints\n",
    "def side_by_side_boxplot(lo_mat1, outErr_mat1,lo_mat2, outErr_mat2, x_labels):\n",
    "    '''\n",
    "    Creates a side-by-side boxplot for log squared error against log p.\n",
    "    lo_mat1: num_sim by num_p matrix, consisting of lo in group 1\n",
    "    outErr_mat1: num_sim by num_p matrix, consisting of out of sample error in group 1\n",
    "    lo_mat2: lo of group 2\n",
    "    outErr_mat2: outErr of group 2\n",
    "    x_labels: list of str, x labels\n",
    "    '''\n",
    "    num_sim = len(lo_mat1)\n",
    "    num_p = len(lo_mat1[0])\n",
    "\n",
    "    error_sq_mat1 = (lo_mat1 - outErr_mat1) ** 2\n",
    "    log_err_sq_mat1 = np.log(error_sq_mat1)\n",
    "    error_sq_mat2 = (lo_mat2 - outErr_mat2) ** 2\n",
    "    log_err_sq_mat2 = np.log(error_sq_mat2)\n",
    "\n",
    "    MSE1 = np.mean(error_sq_mat1, axis=0)\n",
    "    log_MSE1 = np.log(MSE1)\n",
    "    MSE2 = np.mean(error_sq_mat2, axis=0)\n",
    "    log_MSE2 = np.log(MSE2)\n",
    "\n",
    "    # Create a figure and axis object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create boxplots for each age group\n",
    "    ax.boxplot(log_err_sq_mat1, positions=np.arange(num_p)+0.8, widths=0.3)\n",
    "    ax.boxplot(log_err_sq_mat2, positions=np.arange(num_p)+1.2, widths=0.3)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xticks(np.arange(1, num_p+1))\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_xlabel('Log p')\n",
    "    ax.set_ylabel('Log squared error between LO and Err_out')\n",
    "    #ax.set_title('Absolute Error against P (log scale)')\n",
    "\n",
    "    # Fit a linear regression model\n",
    "    x_p = np.arange(1, num_p+1).reshape(-1, 1)\n",
    "    model1 = LinearRegression().fit(x_p, log_MSE1)\n",
    "    model2 = LinearRegression().fit(x_p, log_MSE2)\n",
    "\n",
    "    # Plot regression line for MSE against log(p)\n",
    "    regression_line1 = model1.predict(x_p)\n",
    "    ax.plot(np.arange(num_p)+0.8, regression_line1, color='red', label='Group 1')\n",
    "    regression_line2 = model2.predict(x_p)\n",
    "    ax.plot(np.arange(num_p)+1.2, regression_line2, color='blue', label='Group 2')\n",
    "    print('group 1 slope:', model1.coef_, 'group 2 slope:', model2.coef_)\n",
    "\n",
    "    #Scatterplot of the MSEs\n",
    "    ax.scatter(np.arange(num_p)+0.8, log_MSE1, marker = 'd', color = 'red')\n",
    "    ax.scatter(np.arange(num_p)+1.2, log_MSE2, marker = 'd', color = 'blue')\n",
    "\n",
    "    # Show the legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlBD87spXViG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lasso_noncons = pd.read_csv('lin_elnet_nocons.csv',header = None)\n",
    "lasso_poscons = pd.read_csv('lin_elnet_poscons.csv', header = None)\n",
    "\n",
    "lomat_lasso_noncons = lasso_noncons.iloc[np.arange(0,241),np.arange(0,5)].to_numpy()\n",
    "outErrmat_lasso_noncons = lasso_noncons.iloc[np.arange(0,241),np.arange(5,10)].to_numpy()\n",
    "\n",
    "lomat_lasso_poscons = lasso_poscons.iloc[np.arange(0,241),np.arange(0,5)].to_numpy()\n",
    "outErrmat_lasso_poscons = lasso_poscons.iloc[np.arange(0,241),np.arange(5,10)].to_numpy()\n",
    "\n",
    "x_labels= ['1','2','3','4','5']\n",
    "\n",
    "lo_mat1 = lomat_lasso_noncons\n",
    "outErr_mat1 = outErrmat_lasso_noncons\n",
    "\n",
    "lo_mat2 = lomat_lasso_poscons\n",
    "outErr_mat2 = outErrmat_lasso_poscons\n",
    "\n",
    "num_sim = len(lo_mat1)\n",
    "num_p = len(lo_mat1[0])\n",
    "\n",
    "error_sq_mat1 = (lo_mat1 - outErr_mat1) ** 2\n",
    "log_err_sq_mat1 = np.log(error_sq_mat1)\n",
    "error_sq_mat2 = (lo_mat2 - outErr_mat2) ** 2\n",
    "log_err_sq_mat2 = np.log(error_sq_mat2)\n",
    "\n",
    "MSE1 = np.mean(error_sq_mat1, axis=0)\n",
    "log_MSE1 = np.log(MSE1)\n",
    "MSE2 = np.mean(error_sq_mat2, axis=0)\n",
    "log_MSE2 = np.log(MSE2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create boxplots for each age group\n",
    "ax.boxplot(log_err_sq_mat1, positions=np.arange(num_p)+0.8, widths=0.3)\n",
    "ax.boxplot(log_err_sq_mat2, positions=np.arange(num_p)+1.2, widths=0.3)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xticks(np.arange(1, num_p+1))\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_xlabel('Log p')\n",
    "ax.set_ylabel('Log squared error between LO and Err_out')\n",
    "#ax.set_title('Absolute Error against P (log scale)')\n",
    "\n",
    "# Fit a linear regression model\n",
    "x_p = np.arange(1, num_p+1).reshape(-1, 1)\n",
    "model1 = LinearRegression().fit(np.log(p_), log_MSE1)\n",
    "model2 = LinearRegression().fit(np.log(p_), log_MSE2)\n",
    "\n",
    "# Plot regression line for MSE against log(p)\n",
    "regression_line1 = model1.predict(x_p)\n",
    "ax.plot(np.arange(num_p)+0.8, regression_line1, color='red', label='No constraint')\n",
    "regression_line2 = model2.predict(x_p)\n",
    "ax.plot(np.arange(num_p)+1.2, regression_line2, color='blue', label='Positive constraint')\n",
    "print('group 1 slope:', model1.coef_, 'group 2 slope:', model2.coef_)\n",
    "\n",
    "#Scatterplot of the MSEs\n",
    "ax.scatter(np.arange(num_p)+0.8, log_MSE1, marker = 'd', color = 'red')\n",
    "ax.scatter(np.arange(num_p)+1.2, log_MSE2, marker = 'd', color = 'blue')\n",
    "\n",
    "# Show the legend\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(lo_mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
